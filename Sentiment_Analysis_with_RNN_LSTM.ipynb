{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPpiz2BFevAX9z6t9zCySDM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takhanhvy/-Sentiment-Analysis-with-RNN-LSTM/blob/main/Sentiment_Analysis_with_RNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis for mental health with RNN & NLP"
      ],
      "metadata": {
        "id": "qWR2ApWI48os"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Réalisé par : TA Khanh Vy**  \n",
        "Dans le cadre du cours \"Natural Language Processing\" du M1 Mastère Data Engineering (Efrei) dirigé par Mme. Sarah Malaeb"
      ],
      "metadata": {
        "id": "nydDA3J542mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contexte"
      ],
      "metadata": {
        "id": "UzHQq4fQ8fmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce notebook vise à mener une analyse sentimentale à partir des posts sur les réseaux sociaux dans le but d'identifier automatiquement et catégoriser les sentiments exprimés dans les contenus publiés sur les réseaux sociaux."
      ],
      "metadata": {
        "id": "54pavCK88_8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L’objectif est de développer un modèle de Deep Learning basé sur un réseau neuronal récurrent (RNN/LSTM) capable de prédire le sentiment à partir des textes dans les postes publiés."
      ],
      "metadata": {
        "id": "XmrxbneS94xF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce modèle doit classer chaque texte dans l’une des \" catégories suivantes :\n",
        "- Positive  \n",
        "- Negative  \n",
        "- Neutral  "
      ],
      "metadata": {
        "id": "jmS-2Jkz-Day"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Jeu de données utilisé :**"
      ],
      "metadata": {
        "id": "gIQfikd3-90y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le dataset provient de Kaggle : [Sentiment Analysis](https://www.kaggle.com/datasets/mdismielhossenabir/sentiment-analysis).  \n",
        "\n",
        "Les colonnes principales sont :\n",
        "- `text` : le texte de la publication\n",
        "- `sentiment` : l’étiquette de sentiment associée"
      ],
      "metadata": {
        "id": "RQVIAco0_BwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importation des bibliothèques\n"
      ],
      "metadata": {
        "id": "Rl_0houO_amI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nettoyage et prétraitement des données\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# téléchargement des stopwords et stemmatisation\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# entrainement du modèle RNN/LSTM\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# évaluation de la performance du modèle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "\n",
        "# création des combinaisons d'hyperparamètres\n",
        "from itertools import product"
      ],
      "metadata": {
        "id": "mNLUnd_7_9o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement des données"
      ],
      "metadata": {
        "id": "A4vkIZ2iATzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# charger le fichier csv\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "QKyImiACAIkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lire des données\n",
        "df = pd.read_csv(\"sentiment_analysis.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1_cDzkW_AXYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse exploratoire"
      ],
      "metadata": {
        "id": "VsfYq2vlNros"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution des sentiments\n",
        "sentiments = df[\"sentiment\"].value_counts()\n",
        "\n",
        "plt.figure()\n",
        "sentiments.plot(kind=\"bar\")\n",
        "plt.title(\"Répartition des classes\")\n",
        "plt.xlabel(\"Classe\")\n",
        "plt.ylabel(\"Nombre d'exemples\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "24fVi9P6Nllo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution de la longeur du texte\n",
        "text_length = df['text'].apply(len)\n",
        "plt.hist(text_length)\n",
        "plt.title(\"Text Length Distribution\")\n",
        "plt.xlabel(\"Text length\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XqPqoBU7Nz1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nettoyage et prétraitement des données"
      ],
      "metadata": {
        "id": "5Bb50qWiBUfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vérifier les valeurs nulles\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "X8SKMHGhBadu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nettoyer les données\n",
        "\n",
        "# initializer lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # conversion en miniscule\n",
        "    text = text.lower()\n",
        "    # suppression des charactères spéciaux, des chiffres et de la punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # suppression des urls\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    # tokenization simple (pour nettoyage)\n",
        "    words = text.split()\n",
        "    # supprimer stopwords and appliquer lemmatization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    # appliquer le stemming\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "rPfAyNx5DkCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# appliquer le nettoyage\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "0MpLQvGfP6kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Division du jeu de données en train et test"
      ],
      "metadata": {
        "id": "YmstqeCDQtb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# features et labels\n",
        "X = df['processed_text']\n",
        "y = df['sentiment']"
      ],
      "metadata": {
        "id": "Q9NmzLTGQ1Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder les labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "NkkZoriwQ_Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# division en train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)"
      ],
      "metadata": {
        "id": "HHtHbVQTRF98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrainement du modèle RNN"
      ],
      "metadata": {
        "id": "INEIhms9vMWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization pour RNN et LSTM\n",
        "max_words = 5000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "nXnfdjhHHQED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer les textes en vecteurs numériques\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "sSi4wHhCTOyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding pour avoir la même longeur pour chaque texte\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)"
      ],
      "metadata": {
        "id": "V_gFQ14XLFgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convertir les labels en catégories\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "aoNnIxNsTq5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contruire le modèle RNN simple\n",
        "rnn_model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
        "    SimpleRNN(64, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(3, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "rV4gl27SUVGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compiler avec Adam\n",
        "rnn_model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "y5OKiD5x0Que"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraîner le modèle\n",
        "history = rnn_model.fit(X_train_pad, y_train_cat, epochs=10, batch_size=32,   validation_data=(X_test_pad, y_test_cat), verbose=1)"
      ],
      "metadata": {
        "id": "TcqnDA4L1jjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test et évaluation de la performance du modèle RNN"
      ],
      "metadata": {
        "id": "f2FQqBDH2Fp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prédictions sur le test\n",
        "y_pred_rnn = rnn_model.predict(X_test_pad)"
      ],
      "metadata": {
        "id": "OTFo1JC-2JuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convertir les probabilités en classes prédites\n",
        "y_pred_rnn_classes = np.argmax(y_pred_rnn, axis=1)\n",
        "\n",
        "# Évaluation : accuracy, f1 score, recall_score\n",
        "accuracy = accuracy_score(y_test, y_pred_rnn_classes)\n",
        "f1 = f1_score(y_test, y_pred_rnn_classes, average='weighted')\n",
        "\n",
        "print('RNN Model Performance:')\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'F1-Score: {f1:.4f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred_rnn_classes, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "lWs9NO28Ws9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matrice de confusion\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_rnn_classes), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title('RNN Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zyaug5jmXpbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le modèle RNN obtient environ 66% de bonnes prédictions, ce qui est correct pour un RNN simple sur un dataset textuel brut. Le score F1 proche de l’accuracy indique aussi que la performance est assez équilibrée entre les classes."
      ],
      "metadata": {
        "id": "hjRZETV9BymX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si on fait une analyse par classe :  \n",
        "- `negative` : Le modèle a du mal à identifier les messages négatifs. Il confond souvent cette classe avec les neutres. Il y a 11 prédictions correctes pour \"negative\" et 12 erreurs vers “neutral”. Cela confirme la faiblesse observée dans le recall négatif (0.41).\n",
        "- `neutral` : C’est la classe la mieux reconnue, avec 34 prédictions correcte qui est un très bon score, et quelques erreurs (4 vers énegative\" et 2 vers \"positive\". Le modèle prédit souvent neutral, ce qui explique son recall très élevé (85%). Il a tendance à prédire “neutre” dès qu'il n’est pas certain et cette classe est possiblement sur-apprise par le modèle.\n",
        "- `positive` : La classe positive fonctionne bien en précision (78%) avec 21 bonnes prédictions, ce qui signifie que lorsque le modèle prédit positif, il se trompe peu."
      ],
      "metadata": {
        "id": "39id-RV0CSt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Globalement, le modèle RNN pour ce cas d'usage semble biaisé vers la classe \"neutral\" (comme souvent avec des RNN peu profonds). Cela pourra indiquer que le modèle préfère jouer \"safe\" et classer les textes dans la catégorie la plus “facile”"
      ],
      "metadata": {
        "id": "dQ3yG7pWC-9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualiser les historique d'entraînement : courbes accuracy/loss RNN\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('RNN Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('RNN Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ioG0fpMlYjxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Courbe d’Accuracy :**  \n",
        "L’accuracy d’entraînement monte très rapidement jusqu’à 100%, ce qui signifie que le modèle mémorise très facilement les exemples du training set. En revanche, l’accuracy de validation plafonne autour de 0.70 et évolue très lentement. L’écart entre les deux courbes augmente de façon continue à partir des premières epochs.  \n",
        "=> Cela permet de révéler un surapprentissage (overfitting) clair et précoce.\n",
        "Le modèle apprend à reconnaître parfaitement les données du training set, mais ne généralise pas bien sur de nouvelles données.\n"
      ],
      "metadata": {
        "id": "9SDSoYvxFqU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Courbe de Loss :**  \n",
        "La train loss diminue fortement de 0.22 -> 0.02. La validation loss diminue aussi mais beaucoup moins et avec un plateau (≈0.15). La validation loss reste systématiquement plus haute que la train loss.  \n",
        "=> Encore une fois, on peut observer un phénomène de surapprentissage. La différence entre train loss et validation loss indique que le modèle apprend trop bien les données d’entraînement, mais ne capture pas de manière robuste les motifs généralisables."
      ],
      "metadata": {
        "id": "ehDsP4DoGjlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variantes LSTM"
      ],
      "metadata": {
        "id": "tf_Fm6OK5pcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construire le modèle LSTM\n",
        "lstm_model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128, input_length=max_words),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# compiler\n",
        "lstm_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
        "\n",
        "# entraîner\n",
        "history_lstm = lstm_model.fit(X_train_pad, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test_cat), verbose=1)"
      ],
      "metadata": {
        "id": "i3Bo9qMm5uyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# évaluer la performance du modèle LSTM\n",
        "\n",
        "y_pred_lstm = lstm_model.predict(X_test_pad)\n",
        "y_pred_lstm_classes = np.argmax(y_pred_lstm, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_lstm_classes )\n",
        "f1 = f1_score(y_test, y_pred_lstm_classes , average='weighted')\n",
        "\n",
        "print('LSTM Model Performance:')\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'F1-Score: {f1:.4f}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred_lstm_classes, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "EH3G7Fep504V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matrice de confusion\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_lstm_classes), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.title('LSTM Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bdp2EC1m6KRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le modèle LSTM améliore nettement les performances par rapport au RNN simple. Avec une accuracy de 74 % et un F1-score de 0.73, il parvient à mieux distinguer les trois classes de sentiment. Les scores de précision et de rappel sont également plus équilibrés : le LSTM identifie mieux les messages négatifs et positifs, tout en conservant une excellente performance sur la classe neutre. Globalement, il généralise mieux aux données de test et réduit les biais observés précédemment.\n",
        "\n",
        "La matrice de confusion confirme cette amélioration : les erreurs les plus fréquentes du RNN (notamment la confusion \"negative\" -> \"neutral\") sont nettement réduites. Le LSTM classe correctement la majorité des messages dans chaque catégorie, avec une répartition des erreurs plus homogène et moins systématique. Cette meilleure capacité de séparation montre que le LSTM capture mieux le contexte des phrases, ce qui le rend plus efficace pour l'analyse de sentiments."
      ],
      "metadata": {
        "id": "VB1QfruEHNWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualiser les historique d'entraînement : courbes accuracy/loss LSTM\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_lstm.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_lstm.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('LSTM Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_lstm.history['loss'], label='Train Loss')\n",
        "plt.plot(history_lstm.history['val_loss'], label='Validation Loss')\n",
        "plt.title('LSTM Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0ccDsw9w73bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les courbes d’accuracy montrent une progression régulière de l’apprentissage : l’accuracy d’entraînement augmente de façon stable jusqu’à dépasser 95 %, tandis que l’accuracy de validation progresse également mais de manière plus modérée, se stabilisant autour de 73–75 %. Cela indique que le modèle apprend efficacement les motifs du dataset tout en parvenant à généraliser correctement, même si un léger écart persiste entre les deux courbes. Par rapport au RNN simple, la validation accuracy est plus élevée et surtout plus stable, ce qui confirme une meilleure capacité du LSTM à capturer la structure séquentielle du texte.\n",
        "\n",
        "Les courbes de loss confirment cette observation : la training loss diminue fortement, ce qui montre un apprentissage performant, tandis que la validation loss suit aussi une tendance descendante mais reste plus élevée et légèrement fluctuante. Cela suggère une petite dose d’overfitting, mais nettement moins prononcée que pour le RNN. Globalement, ces courbes indiquent que le LSTM converge bien et généralise mieux, tout en profitant d’une architecture plus robuste pour traiter des données textuelles complexes."
      ],
      "metadata": {
        "id": "n0a87OkBJ97D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimisation d’hyperparamètres"
      ],
      "metadata": {
        "id": "JcbwLnXv8XHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On teste plusieurs combinaisons :\n",
        "- Nombre d’unités (taille cachée)\n",
        "- Nombre de couches\n",
        "- Taux d’apprentissage\n",
        "- Taille du batch\n",
        "- Taux de dropout\n",
        "- Longueur de séquence / fenêtre d’entrée"
      ],
      "metadata": {
        "id": "qj6UaJkT8eNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 1) Fonction qui construit un modèle selon hyperparamètres\n",
        "# =========================================================\n",
        "def build_model(rnn_type=\"LSTM\", units=64, n_layers=1, dropout=0.2, lr=1e-3, max_len=100):\n",
        "    \"\"\"\n",
        "    Construit un modèle LSTM configurable.\n",
        "    - rnn_type : \"LSTM\"\n",
        "    - units : nb d'unités par couche récurrente\n",
        "    - n_layers : profondeur (1 = simple, >1 = empilé)\n",
        "    - dropout : taux dropout\n",
        "    - lr : learning rate Adam\n",
        "    - max_len : longueur de séquence (important pour input_length)\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
        "\n",
        "    # Ajout des couches récurrentes\n",
        "    for i in range(n_layers):\n",
        "        return_seq = (i < n_layers - 1)  # True sauf dernière couche\n",
        "        model.add(LSTM(units, return_sequences=return_seq))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "    # Couche de sortie : 3 classes -> softmax\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # compiler avec optimiseur 'adam' et la fonction de perte calculant l’erreur quadratique moyenne (MSE)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=lr),\n",
        "        loss=\"mse\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 2) Définition de l’espace de recherche\n",
        "# =========================================================\n",
        "param_grid = {\n",
        "    \"rnn_type\": [\"LSTM\"],\n",
        "    \"units\": [32, 64, 128], # Nombre d’unités\n",
        "    \"n_layers\": [1, 2],     # Nombre de couches\n",
        "    \"dropout\": [0.2, 0.4],  # Taux de dropout\n",
        "    \"lr\": [1e-3, 5e-4],     # Taux d'apprentissage\n",
        "    \"batch_size\": [32, 64], # Taille du batch\n",
        "    \"max_len\": [80, 120]    # Longueur de séquence\n",
        "}\n",
        "\n",
        "grid_list = list(product(\n",
        "    param_grid[\"rnn_type\"],\n",
        "    param_grid[\"units\"],\n",
        "    param_grid[\"n_layers\"],\n",
        "    param_grid[\"dropout\"],\n",
        "    param_grid[\"lr\"],\n",
        "    param_grid[\"batch_size\"],\n",
        "    param_grid[\"max_len\"]\n",
        "))\n",
        "\n",
        "print(\"Nombre total de combinaisons testées :\", len(grid_list))\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3) Boucle d’entraînement / évaluation\n",
        "# =========================================================\n",
        "results = []\n",
        "\n",
        "for idx, (rnn_type, units, n_layers, dropout, lr, batch_size, max_len_test) in enumerate(grid_list, 1):\n",
        "\n",
        "    print(f\"\\n--- Test {idx}/{len(grid_list)} | {rnn_type}, units={units}, layers={n_layers}, \"\n",
        "          f\"dropout={dropout}, lr={lr}, batch={batch_size}, max_len={max_len_test} ---\")\n",
        "\n",
        "    # Re-padding selon la longueur de séquence testée\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq  = tokenizer.texts_to_sequences(X_test)\n",
        "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_len_test)\n",
        "    X_test_pad  = pad_sequences(X_test_seq, maxlen=max_len_test)\n",
        "\n",
        "    # Build + train\n",
        "    model = build_model(rnn_type=rnn_type, units=units, n_layers=n_layers,\n",
        "                        dropout=dropout, lr=lr, max_len=max_len_test)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_pad, y_train_cat,\n",
        "        epochs=5,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.2,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Prédiction + métriques\n",
        "    y_pred = model.predict(X_test_pad, verbose=0)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test_cat, axis=1)\n",
        "\n",
        "    acc = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    f1  = f1_score(y_true_classes, y_pred_classes, average=\"weighted\")\n",
        "    test_loss, test_acc = model.evaluate(X_test_pad, y_test_cat, verbose=0)\n",
        "\n",
        "    results.append({\n",
        "        \"rnn_type\": rnn_type,\n",
        "        \"units\": units,\n",
        "        \"n_layers\": n_layers,\n",
        "        \"dropout\": dropout,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"max_len\": max_len_test,\n",
        "        \"test_loss\": test_loss,\n",
        "        \"test_acc\": test_acc,\n",
        "        \"f1_score\": f1\n",
        "    })\n",
        "\n"
      ],
      "metadata": {
        "id": "Y32Bxmeu-hMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 4) Résultats sous forme de tableau trié\n",
        "# =========================================================\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values(by=\"f1_score\", ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 meilleures combinaisons :\")\n",
        "display(results_df.head(10))\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 5) Visualisation simple : F1 en fonction des configs\n",
        "# =========================================================\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(results_df[\"f1_score\"].values, marker=\"o\")\n",
        "plt.title(\"F1-score des combinaisons testées (triées)\")\n",
        "plt.xlabel(\"Combinaisons (triées)\")\n",
        "plt.ylabel(\"F1-score (weighted)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dIJR0OBUKM2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les résultats du tuning montrent que les meilleures performances sont obtenues avec des architectures LSTM relativement simples mais suffisamment expressives. Les configurations les plus efficaces utilisent généralement 128 unités, 1 seule couche, un dropout faible (0.2), un learning rate standard (0.001) et une taille de batch de 32. Ces combinaisons atteignent un F1-score maximal d’environ 0.61, ce qui reste inférieur au LSTM initial (≈0.73), confirmant que le modèle de base était mieux calibré pour ce dataset. On observe également que la longueur de séquence optimale se situe entre 80 et 120 tokens, ce qui montre que des séquences trop longues n’apportent pas de gain significatif.\n",
        "\n",
        "La courbe des F1-scores triés illustre un déclin progressif et stable, typique des grilles d’hyperparamètres où quelques configurations seulement offrent de bons compromis. La majorité des combinaisons obtiennent des scores entre 0.35 et 0.50, ce qui indique que des hyperparamètres mal adaptés dégradent rapidement la capacité du modèle à généraliser. Globalement, cette exploration confirme que le choix des hyperparamètres influence fortement la performance, mais montre également que, dans ce cas précis, les configurations testées n’ont pas surpassé le modèle LSTM initial, confirmant que celui-ci représentait déjà un bon équilibre entre complexité et généralisation."
      ],
      "metadata": {
        "id": "uuHIQGXxLzzD"
      }
    }
  ]
}